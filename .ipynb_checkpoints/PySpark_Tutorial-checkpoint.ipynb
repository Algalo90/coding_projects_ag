{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Covering the basics of SPARK and start working with PySpark\n",
    "### Info Sources:\n",
    "    - https://www.youtube.com/watch?v=QUiAc3rWtMA&t=1s - 11:30\n",
    "\n",
    "\n",
    "### Apache Spark:\n",
    "-  Spark Core is the base of the engine and contains basic functionalities\n",
    "- Spark works with RDD (Resilient Distributed Datasets). Each dataset is divided in partitions computable in different nodes of a cluster.\n",
    "\n",
    "#### Spark SQL:\n",
    "- Provides API's to work with structured data\n",
    "- Allows the manipulation of data via SQL commands\n",
    "- Suports various data formats: CSV, JSON, Parquet, Hive, Cassandra...\n",
    "\n",
    "#### Spark Streaming:\n",
    "- Enables processing of live streams of data with very low latency\n",
    "- divides imput data streams into batches\n",
    "- for example, real time processin of logs of an app server or tweets from twitter\n",
    "\n",
    "#### Spark MLib:\n",
    "- Machine Learning and AI built-in libraries\n",
    "- Data preprocessing + classificaton, regression, cluestring....\n",
    "\n",
    "#### Spark GraphX:\n",
    "- Library for Graph manipulation and computations for big data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['SparkSchema'](Spark_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark provides a layered architecture\n",
    "- All layers and components are loosely couples\n",
    "- A Driver Program (in a SparkContext) runs on the master node of the Spark Cluster\n",
    "- The cluster manager allows to retrieve and work with data from different sources (nodes, cloud, etc...)\n",
    "- Translate the RDD's into the execution graphs, translate the RDD's figures and numbers into graphs.\n",
    "\n",
    "### The Role of an Executor in spark:\n",
    "- Every application needs its won executor process\n",
    "- An executor performs the data processing\n",
    "- it reads data from and writes data to externar sources\n",
    "- interacts with storage systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Deployment Modes:\n",
    "- The Standalone Spark works right so we don't need a third party cluster manager.\n",
    "- Mesos: Can replace the spark cluster manager\n",
    "- Spark on Hadoop YARN: That can enhance the processing capabilities of spark.\n",
    "- Amazon EC2(Elastic cloud computing): We can launch a cluster on Amazon EC2 in 5 min and it accelerates the speed of Spark\n",
    "- Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Applications on YARN:\n",
    "- Spark is preconfigured for YARN\n",
    "- YARN controls resource management, scheduling and security when we run Spark\n",
    "\n",
    "### Cluster Deployment Mode:\n",
    "- Spark runs insida an Application Master managed by YARN\n",
    "- A single process in a YARN container is responsible for driving the application and requestin resources from YARN\n",
    "### Client Deployment Mode\n",
    "- The driver runs on the host where the job is submitted\n",
    "- To request executor containers from YARN, the ApplicationMaster is used\n",
    "- The client communicates with those containers to schedule the work once they start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Shell\n",
    "- Provides a simple way to learn the API\n",
    "- Every SparkContext launches a web UI (User Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-49d7c4e178f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
